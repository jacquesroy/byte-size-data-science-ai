{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    " <IMG SRC=\\\"https://github.com/jacquesroy/byte-size-data-science/raw/master/images/Banner.png\\\" ALT=\\\"BSDS Banner\\\" WIDTH=1195 HEIGHT=200>\n",
    " -->\n",
    "![Banner](./images/Banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example on the TensorFlow [home page](https://www.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries used in this notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset\n",
    "Keep a version of the original integer arrays before making them floats\n",
    "\n",
    "Each image is a 28x28 array which gives an input of 784 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train0, y_train),(x_test0, y_test) = mnist.load_data()\n",
    "# The 28x28 arrays are integers between 0 and 255\n",
    "# Convert to real numbers between 0 and 1\n",
    "x_train, x_test = x_train0 / 255.0, x_test0 / 255.0\n",
    "\n",
    "print(\"Size of the training set: {}\".format(x_train.shape))\n",
    "print(\"Size of the testing  set: {}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "1. input layer: Flatten the array into a vector\n",
    "2. Create a densely connected hidden layer with 128 nodes and using a ReLU activation function\n",
    "3. Set a dropout of 20%. This means that 20% of the values are set to 0 randomly to make sure the model does not depend on specific nodes too strongly.\n",
    "4. Connect the hidden layer to 10 output value using a softmax activation function\n",
    "\n",
    "![Neural Network high-level diagram](./images/NN_MNISTDiagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "#\n",
    "# UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, \n",
    "# prefer using an `Input(shape)` object as the first layer in the model instead.\n",
    "#\n",
    "model = keras.models.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28, 28)), # Flattens returns 784 input values\n",
    "  # keras.layers.Input(shape=(28, 28) ), \n",
    "  keras.layers.Dense(128, activation='relu'), #, input_shape=(28x28)),\n",
    "  keras.layers.Dropout(0.2),\n",
    "  keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an numerical representation of a training digit\n",
    "# Better display when using the integers instead of the float values\n",
    "df = pd.DataFrame(x_train0[0])\n",
    "print(df.to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image itself, and the value it represents, using matplotlib\n",
    "plt.imshow(x_train[0], cmap='gray')\n",
    "plt.title('Label: ' + str(y_train[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some information about the model (before training)\n",
    "print(f\"Model input shape: {model.input_shape}\")\n",
    "print(f\"Number of layers: {len(model.layers)}\")\n",
    "print(f\"Layer names: \" + \", \".join([type(layer).__name__ for layer in model.layers]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = model.get_weights()\n",
    "w_list[0][:20,0:1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some model weights\n",
    "layer_names = [type(layer).__name__ for layer in model.layers]\n",
    "layer_names.append(\"output\")\n",
    "# print(layer_names)\n",
    "w_list = model.get_weights()\n",
    "nb_layers = len(w_list)\n",
    "print(f\"weights matrix length (number of layers): {nb_layers}\")\n",
    "for ix in range(nb_layers):\n",
    "    print(f\"Layer \\033[1;32m{layer_names[ix]} to {layer_names[ix+1]}\\033[0m shape: {w_list[ix].shape}\")\n",
    "    # print(f\"Layer {ix} shape: {w_list[ix].shape}\")\n",
    "print(\"\\nFirst 20 pixels (of 784) initial weights for the first neuron of the hidden layer:\")\n",
    "print(w_list[0][:20,0:1].flatten()) # get list[0], get the 784 values form the first neuron\n",
    "\n",
    "print(\"\\nFirst 20 neurons (of 128) initial weights for the first pixel going to each neuron of the hidden layer:\")\n",
    "print(w_list[0][0:1, :20].flatten()) # get list[0], get the 784 values form the first neuron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Each epoch processes the entire dataset in batches. Default batch size is 32\n",
    "# FYI 1875 batches of 32 is 60,000 values\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model. Metrics: 'loss', 'accuracy'\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])\n",
    "# Show the 10 output values for the first test digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the precision to make it more human-readable\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{:.4f}\".format(x)})\n",
    "print(\"  0      1      2      3      4      5      6      7      8      9\")\n",
    "print(predictions[0])\n",
    "np.set_printoptions(formatter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(predictions[0])\n",
    "# type(np.where( predictions[0] == max(predictions[0]) ))\n",
    "for ix in range(0, 10) :\n",
    "    prediction = np.where( predictions[ix] == max(predictions[ix]) )[0][0].item()\n",
    "    expected = y_test[ix].item()\n",
    "    if expected == prediction :\n",
    "        print(f\"\\033[32mPrediction: {prediction}, expected: {expected}\\033[0m\")\n",
    "    else :\n",
    "        print(f\"\\033[31mPrediction: {prediction}, expected: {expected}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    model, to_file='./model.png', show_shapes=True, show_dtype=False,\n",
    "    show_layer_names=False, rankdir='TB', expand_nested=True, dpi=96,\n",
    "    layer_range=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
